{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "import process_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_header(file_path, expected_columns=None):\n",
    "    \"\"\"\n",
    "    Check if a CSV file has a header row by examining the first row's values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read only the first row\n",
    "    df = pd.read_csv(file_path, nrows=1)\n",
    "    first_row = df.columns.tolist()\n",
    "    \n",
    "    # If expected_columns is provided, check for case-insensitive match\n",
    "    if expected_columns:\n",
    "        return all(col.lower() in [e.lower() for e in expected_columns] for col in first_row)\n",
    "    \n",
    "    # Otherwise, assume header if all values are strings and not numeric\n",
    "    return all(isinstance(col, str) and not col.replace('.', '', 1).isdigit() for col in first_row)\n",
    "\n",
    "def is_empty(file_path):\n",
    "    # Check if file is empty by size\n",
    "    if os.path.getsize(file_path) == 0:\n",
    "        return True\n",
    "    \n",
    "    # Try to read the CSV to check for data rows\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, nrows=1)\n",
    "        return df.empty or len(pd.read_csv(file_path)) == 0\n",
    "    except pd.errors.EmptyDataError:\n",
    "        return True  # File is empty or invalid\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return True  # Treat other errors as empty to avoid crashing\n",
    "\n",
    "def show_head(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Print first 3 rows\n",
    "    print(df.head(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_folder_path = \"../data/Kraken_OHLCVT/\"\n",
    "out_folder_path = \"../data/Kraken_cleaned/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_data.add_colnames(in_folder_path + os.listdir(in_folder_path)[0])\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/13/l50f8245569cx9x16ghycw3m0000gn/T/ipykernel_63860/2700762240.py:1: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df.resample('H', on='timestamp').agg({\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>trades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-08-26 15:00:00</td>\n",
       "      <td>0.06446</td>\n",
       "      <td>0.06446</td>\n",
       "      <td>0.06446</td>\n",
       "      <td>0.06446</td>\n",
       "      <td>152.840000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022-08-27 03:00:00</td>\n",
       "      <td>0.62000</td>\n",
       "      <td>0.62000</td>\n",
       "      <td>0.62000</td>\n",
       "      <td>0.62000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022-08-27 04:00:00</td>\n",
       "      <td>0.06182</td>\n",
       "      <td>0.06187</td>\n",
       "      <td>0.06182</td>\n",
       "      <td>0.06187</td>\n",
       "      <td>552.050000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-08-27 07:00:00</td>\n",
       "      <td>0.61800</td>\n",
       "      <td>0.61800</td>\n",
       "      <td>0.06169</td>\n",
       "      <td>0.06208</td>\n",
       "      <td>1994.523111</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022-08-27 09:00:00</td>\n",
       "      <td>0.06389</td>\n",
       "      <td>0.06473</td>\n",
       "      <td>0.06389</td>\n",
       "      <td>0.06473</td>\n",
       "      <td>182.510000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22752</th>\n",
       "      <td>2025-03-31 15:00:00</td>\n",
       "      <td>0.03531</td>\n",
       "      <td>0.03532</td>\n",
       "      <td>0.03531</td>\n",
       "      <td>0.03532</td>\n",
       "      <td>793.260000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22753</th>\n",
       "      <td>2025-03-31 16:00:00</td>\n",
       "      <td>0.03486</td>\n",
       "      <td>0.03486</td>\n",
       "      <td>0.03478</td>\n",
       "      <td>0.03478</td>\n",
       "      <td>550.327625</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22754</th>\n",
       "      <td>2025-03-31 17:00:00</td>\n",
       "      <td>0.03499</td>\n",
       "      <td>0.03500</td>\n",
       "      <td>0.03499</td>\n",
       "      <td>0.03500</td>\n",
       "      <td>23717.351666</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22757</th>\n",
       "      <td>2025-03-31 20:00:00</td>\n",
       "      <td>0.03483</td>\n",
       "      <td>0.03483</td>\n",
       "      <td>0.03470</td>\n",
       "      <td>0.03470</td>\n",
       "      <td>3400.360000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22759</th>\n",
       "      <td>2025-03-31 22:00:00</td>\n",
       "      <td>0.03434</td>\n",
       "      <td>0.03434</td>\n",
       "      <td>0.03433</td>\n",
       "      <td>0.03433</td>\n",
       "      <td>1266.450000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15635 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp     open     high      low    close        volume  \\\n",
       "0     2022-08-26 15:00:00  0.06446  0.06446  0.06446  0.06446    152.840000   \n",
       "12    2022-08-27 03:00:00  0.62000  0.62000  0.62000  0.62000     50.000000   \n",
       "13    2022-08-27 04:00:00  0.06182  0.06187  0.06182  0.06187    552.050000   \n",
       "16    2022-08-27 07:00:00  0.61800  0.61800  0.06169  0.06208   1994.523111   \n",
       "18    2022-08-27 09:00:00  0.06389  0.06473  0.06389  0.06473    182.510000   \n",
       "...                   ...      ...      ...      ...      ...           ...   \n",
       "22752 2025-03-31 15:00:00  0.03531  0.03532  0.03531  0.03532    793.260000   \n",
       "22753 2025-03-31 16:00:00  0.03486  0.03486  0.03478  0.03478    550.327625   \n",
       "22754 2025-03-31 17:00:00  0.03499  0.03500  0.03499  0.03500  23717.351666   \n",
       "22757 2025-03-31 20:00:00  0.03483  0.03483  0.03470  0.03470   3400.360000   \n",
       "22759 2025-03-31 22:00:00  0.03434  0.03434  0.03433  0.03433   1266.450000   \n",
       "\n",
       "       trades  \n",
       "0           1  \n",
       "12          1  \n",
       "13          2  \n",
       "16          8  \n",
       "18          3  \n",
       "...       ...  \n",
       "22752       2  \n",
       "22753       3  \n",
       "22754       2  \n",
       "22757       2  \n",
       "22759       2  \n",
       "\n",
       "[15635 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.resample('H', on='timestamp').agg({\n",
    "    'open': 'first',\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'close': 'last',\n",
    "    'volume': 'sum',\n",
    "    'trades': 'sum'\n",
    "}).reset_index().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>trades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-08-26 15:59:00</td>\n",
       "      <td>0.06446</td>\n",
       "      <td>0.06446</td>\n",
       "      <td>0.06446</td>\n",
       "      <td>0.06446</td>\n",
       "      <td>152.840000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-08-27 03:54:00</td>\n",
       "      <td>0.62000</td>\n",
       "      <td>0.62000</td>\n",
       "      <td>0.62000</td>\n",
       "      <td>0.62000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-08-27 04:02:00</td>\n",
       "      <td>0.06182</td>\n",
       "      <td>0.06182</td>\n",
       "      <td>0.06182</td>\n",
       "      <td>0.06182</td>\n",
       "      <td>350.610000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-08-27 04:15:00</td>\n",
       "      <td>0.06187</td>\n",
       "      <td>0.06187</td>\n",
       "      <td>0.06187</td>\n",
       "      <td>0.06187</td>\n",
       "      <td>201.440000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-08-27 07:02:00</td>\n",
       "      <td>0.61800</td>\n",
       "      <td>0.61800</td>\n",
       "      <td>0.06169</td>\n",
       "      <td>0.06169</td>\n",
       "      <td>397.346448</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83864</th>\n",
       "      <td>2025-03-31 16:36:00</td>\n",
       "      <td>0.03478</td>\n",
       "      <td>0.03478</td>\n",
       "      <td>0.03478</td>\n",
       "      <td>0.03478</td>\n",
       "      <td>26.760000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83865</th>\n",
       "      <td>2025-03-31 17:10:00</td>\n",
       "      <td>0.03499</td>\n",
       "      <td>0.03500</td>\n",
       "      <td>0.03499</td>\n",
       "      <td>0.03500</td>\n",
       "      <td>23717.351666</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83866</th>\n",
       "      <td>2025-03-31 20:09:00</td>\n",
       "      <td>0.03483</td>\n",
       "      <td>0.03483</td>\n",
       "      <td>0.03483</td>\n",
       "      <td>0.03483</td>\n",
       "      <td>561.100000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83867</th>\n",
       "      <td>2025-03-31 20:24:00</td>\n",
       "      <td>0.03470</td>\n",
       "      <td>0.03470</td>\n",
       "      <td>0.03470</td>\n",
       "      <td>0.03470</td>\n",
       "      <td>2839.260000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83868</th>\n",
       "      <td>2025-03-31 22:35:00</td>\n",
       "      <td>0.03434</td>\n",
       "      <td>0.03434</td>\n",
       "      <td>0.03433</td>\n",
       "      <td>0.03433</td>\n",
       "      <td>1266.450000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83869 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp     open     high      low    close        volume  \\\n",
       "0     2022-08-26 15:59:00  0.06446  0.06446  0.06446  0.06446    152.840000   \n",
       "1     2022-08-27 03:54:00  0.62000  0.62000  0.62000  0.62000     50.000000   \n",
       "2     2022-08-27 04:02:00  0.06182  0.06182  0.06182  0.06182    350.610000   \n",
       "3     2022-08-27 04:15:00  0.06187  0.06187  0.06187  0.06187    201.440000   \n",
       "4     2022-08-27 07:02:00  0.61800  0.61800  0.06169  0.06169    397.346448   \n",
       "...                   ...      ...      ...      ...      ...           ...   \n",
       "83864 2025-03-31 16:36:00  0.03478  0.03478  0.03478  0.03478     26.760000   \n",
       "83865 2025-03-31 17:10:00  0.03499  0.03500  0.03499  0.03500  23717.351666   \n",
       "83866 2025-03-31 20:09:00  0.03483  0.03483  0.03483  0.03483    561.100000   \n",
       "83867 2025-03-31 20:24:00  0.03470  0.03470  0.03470  0.03470   2839.260000   \n",
       "83868 2025-03-31 22:35:00  0.03434  0.03434  0.03433  0.03433   1266.450000   \n",
       "\n",
       "       trades  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           2  \n",
       "...       ...  \n",
       "83864       1  \n",
       "83865       2  \n",
       "83866       1  \n",
       "83867       1  \n",
       "83868       2  \n",
       "\n",
       "[83869 rows x 7 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>trades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-08-26 15:59:00</td>\n",
       "      <td>0.06446</td>\n",
       "      <td>0.06446</td>\n",
       "      <td>0.06446</td>\n",
       "      <td>0.06446</td>\n",
       "      <td>152.840000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-08-27 03:54:00</td>\n",
       "      <td>0.62000</td>\n",
       "      <td>0.62000</td>\n",
       "      <td>0.62000</td>\n",
       "      <td>0.62000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-08-27 04:02:00</td>\n",
       "      <td>0.06182</td>\n",
       "      <td>0.06182</td>\n",
       "      <td>0.06182</td>\n",
       "      <td>0.06182</td>\n",
       "      <td>350.610000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-08-27 04:15:00</td>\n",
       "      <td>0.06187</td>\n",
       "      <td>0.06187</td>\n",
       "      <td>0.06187</td>\n",
       "      <td>0.06187</td>\n",
       "      <td>201.440000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-08-27 07:02:00</td>\n",
       "      <td>0.61800</td>\n",
       "      <td>0.61800</td>\n",
       "      <td>0.06169</td>\n",
       "      <td>0.06169</td>\n",
       "      <td>397.346448</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83864</th>\n",
       "      <td>2025-03-31 16:36:00</td>\n",
       "      <td>0.03478</td>\n",
       "      <td>0.03478</td>\n",
       "      <td>0.03478</td>\n",
       "      <td>0.03478</td>\n",
       "      <td>26.760000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83865</th>\n",
       "      <td>2025-03-31 17:10:00</td>\n",
       "      <td>0.03499</td>\n",
       "      <td>0.03500</td>\n",
       "      <td>0.03499</td>\n",
       "      <td>0.03500</td>\n",
       "      <td>23717.351666</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83866</th>\n",
       "      <td>2025-03-31 20:09:00</td>\n",
       "      <td>0.03483</td>\n",
       "      <td>0.03483</td>\n",
       "      <td>0.03483</td>\n",
       "      <td>0.03483</td>\n",
       "      <td>561.100000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83867</th>\n",
       "      <td>2025-03-31 20:24:00</td>\n",
       "      <td>0.03470</td>\n",
       "      <td>0.03470</td>\n",
       "      <td>0.03470</td>\n",
       "      <td>0.03470</td>\n",
       "      <td>2839.260000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83868</th>\n",
       "      <td>2025-03-31 22:35:00</td>\n",
       "      <td>0.03434</td>\n",
       "      <td>0.03434</td>\n",
       "      <td>0.03433</td>\n",
       "      <td>0.03433</td>\n",
       "      <td>1266.450000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83869 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp     open     high      low    close        volume  \\\n",
       "0     2022-08-26 15:59:00  0.06446  0.06446  0.06446  0.06446    152.840000   \n",
       "1     2022-08-27 03:54:00  0.62000  0.62000  0.62000  0.62000     50.000000   \n",
       "2     2022-08-27 04:02:00  0.06182  0.06182  0.06182  0.06182    350.610000   \n",
       "3     2022-08-27 04:15:00  0.06187  0.06187  0.06187  0.06187    201.440000   \n",
       "4     2022-08-27 07:02:00  0.61800  0.61800  0.06169  0.06169    397.346448   \n",
       "...                   ...      ...      ...      ...      ...           ...   \n",
       "83864 2025-03-31 16:36:00  0.03478  0.03478  0.03478  0.03478     26.760000   \n",
       "83865 2025-03-31 17:10:00  0.03499  0.03500  0.03499  0.03500  23717.351666   \n",
       "83866 2025-03-31 20:09:00  0.03483  0.03483  0.03483  0.03483    561.100000   \n",
       "83867 2025-03-31 20:24:00  0.03470  0.03470  0.03470  0.03470   2839.260000   \n",
       "83868 2025-03-31 22:35:00  0.03434  0.03434  0.03433  0.03433   1266.450000   \n",
       "\n",
       "       trades  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           2  \n",
       "...       ...  \n",
       "83864       1  \n",
       "83865       2  \n",
       "83866       1  \n",
       "83867       1  \n",
       "83868       2  \n",
       "\n",
       "[83869 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8656"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(in_folder_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(in_folder_path + os.listdir(in_folder_path)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1661529540</th>\n",
       "      <th>0.06446</th>\n",
       "      <th>0.06446.1</th>\n",
       "      <th>0.06446.2</th>\n",
       "      <th>0.06446.3</th>\n",
       "      <th>152.84</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1661572440</td>\n",
       "      <td>0.62000</td>\n",
       "      <td>0.62000</td>\n",
       "      <td>0.62000</td>\n",
       "      <td>0.62000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1661572920</td>\n",
       "      <td>0.06182</td>\n",
       "      <td>0.06182</td>\n",
       "      <td>0.06182</td>\n",
       "      <td>0.06182</td>\n",
       "      <td>350.610000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1661573700</td>\n",
       "      <td>0.06187</td>\n",
       "      <td>0.06187</td>\n",
       "      <td>0.06187</td>\n",
       "      <td>0.06187</td>\n",
       "      <td>201.440000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1661583720</td>\n",
       "      <td>0.61800</td>\n",
       "      <td>0.61800</td>\n",
       "      <td>0.06169</td>\n",
       "      <td>0.06169</td>\n",
       "      <td>397.346448</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1661584020</td>\n",
       "      <td>0.06197</td>\n",
       "      <td>0.06197</td>\n",
       "      <td>0.06197</td>\n",
       "      <td>0.06197</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83863</th>\n",
       "      <td>1743438960</td>\n",
       "      <td>0.03478</td>\n",
       "      <td>0.03478</td>\n",
       "      <td>0.03478</td>\n",
       "      <td>0.03478</td>\n",
       "      <td>26.760000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83864</th>\n",
       "      <td>1743441000</td>\n",
       "      <td>0.03499</td>\n",
       "      <td>0.03500</td>\n",
       "      <td>0.03499</td>\n",
       "      <td>0.03500</td>\n",
       "      <td>23717.351666</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83865</th>\n",
       "      <td>1743451740</td>\n",
       "      <td>0.03483</td>\n",
       "      <td>0.03483</td>\n",
       "      <td>0.03483</td>\n",
       "      <td>0.03483</td>\n",
       "      <td>561.100000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83866</th>\n",
       "      <td>1743452640</td>\n",
       "      <td>0.03470</td>\n",
       "      <td>0.03470</td>\n",
       "      <td>0.03470</td>\n",
       "      <td>0.03470</td>\n",
       "      <td>2839.260000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83867</th>\n",
       "      <td>1743460500</td>\n",
       "      <td>0.03434</td>\n",
       "      <td>0.03434</td>\n",
       "      <td>0.03433</td>\n",
       "      <td>0.03433</td>\n",
       "      <td>1266.450000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83868 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1661529540  0.06446  0.06446.1  0.06446.2  0.06446.3        152.84  1\n",
       "0      1661572440  0.62000    0.62000    0.62000    0.62000     50.000000  1\n",
       "1      1661572920  0.06182    0.06182    0.06182    0.06182    350.610000  1\n",
       "2      1661573700  0.06187    0.06187    0.06187    0.06187    201.440000  1\n",
       "3      1661583720  0.61800    0.61800    0.06169    0.06169    397.346448  2\n",
       "4      1661584020  0.06197    0.06197    0.06197    0.06197    999.000000  1\n",
       "...           ...      ...        ...        ...        ...           ... ..\n",
       "83863  1743438960  0.03478    0.03478    0.03478    0.03478     26.760000  1\n",
       "83864  1743441000  0.03499    0.03500    0.03499    0.03500  23717.351666  2\n",
       "83865  1743451740  0.03483    0.03483    0.03483    0.03483    561.100000  1\n",
       "83866  1743452640  0.03470    0.03470    0.03470    0.03470   2839.260000  1\n",
       "83867  1743460500  0.03434    0.03434    0.03433    0.03433   1266.450000  2\n",
       "\n",
       "[83868 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TRUUSD_1.csv'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_file = '/Users/xin/Stanford/MSE 349 Financial Statistics/crypto_data/Kraken_OHLCVT/BTCUSD_Daily_OHLC.csv'\n",
    "#is_empty(btc_file)\n",
    "# Warning: some files are empty\n",
    "CELOEUR_1 = '/Users/xin/Stanford/MSE 349 Financial Statistics/crypto_data/Kraken_OHLCVT/CELOEUR_1.csv'\n",
    "is_empty(CELOEUR_1)\n",
    "show_head(CELOEUR_1) # empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THIS IS THE ONLY file with headers\n",
    "file = '/Users/xin/Stanford/MSE 349 Financial Statistics/crypto_data/Kraken_OHLCVT/BTCUSD_Daily_OHLC.csv'\n",
    "has_header(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: CELOEUR_1.csv is empty\n",
      "Warning: METISUSD_720.csv is empty\n",
      "Warning: RLUSDUSDC_60.csv is empty\n",
      "Warning: CELOEUR_60.csv is empty\n",
      "Warning: ARCUSD_30.csv is empty\n",
      "Warning: USDDEUR_30.csv is empty\n",
      "Warning: TITCOINEUR_60.csv is empty\n",
      "Warning: SUNUSD_15.csv is empty\n",
      "Warning: WCTEUR_60.csv is empty\n",
      "Warning: BABYEUR_720.csv is empty\n",
      "Warning: KERNELUSD_1.csv is empty\n",
      "Warning: USDDEUR_720.csv is empty\n",
      "Warning: PROMPTUSD_15.csv is empty\n",
      "Warning: BNBEUR_1.csv is empty\n",
      "Warning: RLUSDUSDT_30.csv is empty\n",
      "Warning: BNBUSD_1.csv is empty\n",
      "Warning: RLUSDUSD_1440.csv is empty\n",
      "Warning: CELOUSD_240.csv is empty\n",
      "Warning: XRPRLUSD_1440.csv is empty\n",
      "Warning: KERNELEUR_1.csv is empty\n",
      "Warning: HMSTREUR_30.csv is empty\n",
      "Warning: ALCHUSD_30.csv is empty\n",
      "Warning: BNBUSDC_5.csv is empty\n",
      "Warning: JSTUSD_15.csv is empty\n",
      "Warning: ARUSD_15.csv is empty\n",
      "Warning: CELOUSD_1.csv is empty\n",
      "Warning: KUSD_1440.csv is empty\n",
      "Warning: WCTEUR_1440.csv is empty\n",
      "Warning: EDGEUSD_15.csv is empty\n",
      "Warning: BNBEUR_30.csv is empty\n",
      "Warning: ETHAED_30.csv is empty\n",
      "Warning: TRXUSDD_30.csv is empty\n",
      "Warning: HMSTREUR_1.csv is empty\n",
      "Warning: WINUSD_1440.csv is empty\n",
      "Warning: SSVUSD_1440.csv is empty\n",
      "Warning: SUNEUR_720.csv is empty\n",
      "Warning: KERNELEUR_15.csv is empty\n",
      "Warning: PROMPTEUR_1440.csv is empty\n",
      "Warning: XRPRLUSD_30.csv is empty\n",
      "Warning: HMSTRUSD_1.csv is empty\n",
      "Warning: AREUR_720.csv is empty\n",
      "Warning: JSTUSD_240.csv is empty\n",
      "Warning: METISEUR_1440.csv is empty\n",
      "Warning: AREUR_240.csv is empty\n",
      "Warning: JSTUSD_720.csv is empty\n",
      "Warning: EDGEEUR_60.csv is empty\n",
      "Warning: HMSTRUSD_5.csv is empty\n",
      "Warning: TITCOINEUR_1440.csv is empty\n",
      "Warning: TRXUSDD_1440.csv is empty\n",
      "Warning: JSTEUR_1440.csv is empty\n",
      "Warning: METISEUR_30.csv is empty\n",
      "Warning: GUNUSD_30.csv is empty\n",
      "Warning: KERNELUSD_60.csv is empty\n",
      "Warning: FHEEUR_30.csv is empty\n",
      "Warning: BNBUSD_1440.csv is empty\n",
      "Warning: HMSTREUR_5.csv is empty\n",
      "Warning: SUNEUR_240.csv is empty\n",
      "Warning: ALCHUSD_1440.csv is empty\n",
      "Warning: KEUR_720.csv is empty\n",
      "Warning: USDAED_240.csv is empty\n",
      "Warning: USDGEUR_15.csv is empty\n",
      "Warning: GUNUSD_1440.csv is empty\n",
      "Warning: CELOUSD_15.csv is empty\n",
      "Warning: CELOUSD_5.csv is empty\n",
      "Warning: TITCOINUSD_15.csv is empty\n",
      "Warning: XBTUSDQ_30.csv is empty\n",
      "Warning: WINUSD_30.csv is empty\n",
      "Warning: BNBUSDC_1.csv is empty\n",
      "Warning: PROMPTEUR_60.csv is empty\n",
      "Warning: CELOUSD_720.csv is empty\n",
      "Warning: WCTUSD_15.csv is empty\n",
      "Warning: SUNEUR_60.csv is empty\n",
      "Warning: KERNELEUR_5.csv is empty\n",
      "Warning: BNBUSD_5.csv is empty\n",
      "Warning: ARCEUR_1440.csv is empty\n",
      "Warning: USDDEUR_240.csv is empty\n",
      "Warning: BABYEUR_240.csv is empty\n",
      "Warning: BNBEUR_5.csv is empty\n",
      "Warning: SSVUSD_30.csv is empty\n",
      "Warning: RLUSDUSD_30.csv is empty\n",
      "Warning: BABYEUR_30.csv is empty\n",
      "Warning: KERNELUSD_5.csv is empty\n",
      "Warning: BNBUSDC_60.csv is empty\n",
      "Warning: METISUSD_240.csv is empty\n",
      "Warning: AREUR_60.csv is empty\n",
      "Warning: BNBUSDT_30.csv is empty\n",
      "Warning: JSTEUR_60.csv is empty\n",
      "Warning: XBTUSDR_60.csv is empty\n",
      "Warning: CELOEUR_1440.csv is empty\n",
      "Warning: CELOEUR_5.csv is empty\n",
      "Warning: ARCUSD_720.csv is empty\n",
      "Warning: RLUSDUSD_5.csv is empty\n",
      "Warning: WCTUSD_1.csv is empty\n",
      "Warning: FHEEUR_240.csv is empty\n",
      "Warning: JSTEUR_1.csv is empty\n",
      "Warning: ALCHUSD_240.csv is empty\n",
      "Warning: TITCOINUSD_5.csv is empty\n",
      "Warning: XBTUSDR_1440.csv is empty\n",
      "Warning: FHEUSD_30.csv is empty\n",
      "Warning: JSTUSD_1.csv is empty\n",
      "Warning: METISUSD_30.csv is empty\n",
      "Warning: GUNEUR_30.csv is empty\n",
      "Warning: KERNELEUR_60.csv is empty\n",
      "Warning: TITCOINEUR_5.csv is empty\n",
      "Warning: HMSTREUR_240.csv is empty\n",
      "Warning: RLUSDEUR_5.csv is empty\n",
      "Warning: EDGEUSD_240.csv is empty\n",
      "Warning: XBTAED_240.csv is empty\n",
      "Warning: WCTEUR_1.csv is empty\n",
      "Warning: RLUSDEUR_1440.csv is empty\n",
      "Warning: XBTAED_30.csv is empty\n",
      "Warning: WINUSD_240.csv is empty\n",
      "Warning: KEUR_1440.csv is empty\n",
      "Warning: EDGEUSD_60.csv is empty\n",
      "Warning: RLUSDUSDT_1440.csv is empty\n",
      "Warning: RLUSDUSDC_5.csv is empty\n",
      "Warning: GUNEUR_720.csv is empty\n",
      "Warning: ARCEUR_1.csv is empty\n",
      "Warning: ARUSD_60.csv is empty\n",
      "Warning: JSTUSD_60.csv is empty\n",
      "Warning: WCTUSD_1440.csv is empty\n",
      "Warning: SSVUSD_720.csv is empty\n",
      "Warning: PROMPTUSD_1440.csv is empty\n",
      "Warning: SSVEUR_1440.csv is empty\n",
      "Warning: WINEUR_1440.csv is empty\n",
      "Warning: BABYUSD_30.csv is empty\n",
      "Warning: PROMPTUSD_720.csv is empty\n",
      "Warning: BABYEUR_5.csv is empty\n",
      "Warning: RLUSDEUR_30.csv is empty\n",
      "Warning: SSVEUR_30.csv is empty\n",
      "Warning: BABYUSD_5.csv is empty\n",
      "Warning: RLUSDUSD_240.csv is empty\n",
      "Warning: XRPRLUSD_5.csv is empty\n",
      "Warning: PROMPTUSD_60.csv is empty\n",
      "Warning: USDGEUR_5.csv is empty\n",
      "Warning: KERNELUSD_720.csv is empty\n",
      "Warning: WCTEUR_15.csv is empty\n",
      "Warning: SUNUSD_60.csv is empty\n",
      "Warning: ARCUSD_1.csv is empty\n",
      "Warning: TITCOINEUR_15.csv is empty\n",
      "Warning: WINEUR_30.csv is empty\n",
      "Warning: CELOEUR_15.csv is empty\n",
      "Warning: RLUSDUSDC_15.csv is empty\n",
      "Warning: WCTEUR_720.csv is empty\n",
      "Warning: BNBUSD_720.csv is empty\n",
      "Warning: TITCOINEUR_240.csv is empty\n",
      "Warning: BNBUSDC_1440.csv is empty\n",
      "Warning: XBTUSDR_15.csv is empty\n",
      "Warning: JSTEUR_15.csv is empty\n",
      "Warning: AREUR_15.csv is empty\n",
      "Warning: WCTEUR_240.csv is empty\n",
      "Warning: TITCOINEUR_720.csv is empty\n",
      "Warning: BNBUSD_240.csv is empty\n",
      "Warning: METISUSD_1440.csv is empty\n",
      "Warning: ARCUSD_5.csv is empty\n",
      "Warning: BNBUSDC_15.csv is empty\n",
      "Warning: JSTUSD_1440.csv is empty\n",
      "Warning: RLUSDUSD_720.csv is empty\n",
      "Warning: ALCHEUR_30.csv is empty\n",
      "Warning: XRPRLUSD_1.csv is empty\n",
      "Warning: HMSTRUSD_30.csv is empty\n",
      "Warning: USDGEUR_1.csv is empty\n",
      "Warning: TITCOINUSD_1440.csv is empty\n",
      "Warning: KERNELUSD_240.csv is empty\n",
      "Warning: BABYUSD_1.csv is empty\n",
      "Warning: BABYEUR_1.csv is empty\n",
      "Warning: SSVUSD_240.csv is empty\n",
      "Warning: BNBEUR_1440.csv is empty\n",
      "Warning: SUNEUR_15.csv is empty\n",
      "Warning: WCTUSD_60.csv is empty\n",
      "Warning: PROMPTEUR_15.csv is empty\n",
      "Warning: PROMPTUSD_240.csv is empty\n",
      "Warning: USDAED_30.csv is empty\n",
      "Warning: USDDUSD_30.csv is empty\n",
      "Warning: TITCOINUSD_60.csv is empty\n",
      "Warning: ARCEUR_5.csv is empty\n",
      "Warning: GUNEUR_240.csv is empty\n",
      "Warning: ALCHEUR_1440.csv is empty\n",
      "Warning: CELOUSD_60.csv is empty\n",
      "Warning: USDGEUR_60.csv is empty\n",
      "Warning: ARCEUR_30.csv is empty\n",
      "Warning: RLUSDUSDC_1.csv is empty\n",
      "Warning: WINUSD_720.csv is empty\n",
      "Warning: GUNEUR_1440.csv is empty\n",
      "Warning: WCTEUR_5.csv is empty\n",
      "Warning: RLUSDEUR_1.csv is empty\n",
      "Warning: TITCOINEUR_1.csv is empty\n",
      "Warning: HMSTREUR_720.csv is empty\n",
      "Warning: JSTUSD_5.csv is empty\n",
      "Warning: EDGEUSD_720.csv is empty\n",
      "Warning: KERNELUSD_15.csv is empty\n",
      "Warning: TITCOINUSD_1.csv is empty\n",
      "Warning: JSTEUR_5.csv is empty\n",
      "Warning: ALCHUSD_720.csv is empty\n",
      "Warning: WCTUSD_5.csv is empty\n",
      "Warning: RLUSDUSD_1.csv is empty\n",
      "Warning: FHEEUR_720.csv is empty\n",
      "Warning: ARCUSD_1440.csv is empty\n",
      "Warning: EDGEEUR_15.csv is empty\n",
      "Warning: BNBUSD_30.csv is empty\n",
      "Warning: CELOUSD_1440.csv is empty\n",
      "Warning: ARCUSD_240.csv is empty\n",
      "Warning: BNBUSDT_240.csv is empty\n",
      "Warning: WCTUSD_720.csv is empty\n",
      "Warning: TITCOINUSD_240.csv is empty\n",
      "Warning: BNBEUR_720.csv is empty\n",
      "Warning: BNBUSD_15.csv is empty\n",
      "Warning: EDGEEUR_30.csv is empty\n",
      "Warning: SUNEUR_1440.csv is empty\n",
      "Warning: ALCHEUR_5.csv is empty\n",
      "Warning: RLUSDEUR_240.csv is empty\n",
      "Warning: EDGEUSD_1440.csv is empty\n",
      "Warning: KERNELEUR_720.csv is empty\n",
      "Warning: SSVEUR_1.csv is empty\n",
      "Warning: METISEUR_60.csv is empty\n",
      "Warning: GUNUSD_60.csv is empty\n",
      "Warning: KERNELUSD_30.csv is empty\n",
      "Warning: TRXUSDD_1.csv is empty\n",
      "Warning: SSVUSD_1.csv is empty\n",
      "Warning: FHEEUR_60.csv is empty\n",
      "Warning: SSVEUR_720.csv is empty\n",
      "Warning: XBTUSDQ_720.csv is empty\n",
      "Warning: ALCHUSD_5.csv is empty\n",
      "Warning: PROMPTEUR_720.csv is empty\n",
      "Warning: GUNUSD_720.csv is empty\n",
      "Warning: RLUSDUSDT_1.csv is empty\n",
      "Warning: TRXUSDD_720.csv is empty\n",
      "Warning: ARCEUR_15.csv is empty\n",
      "Warning: GUNEUR_1.csv is empty\n",
      "Warning: WINEUR_240.csv is empty\n",
      "Warning: XBTUSDQ_60.csv is empty\n",
      "Warning: USDDUSD_15.csv is empty\n",
      "Warning: WINUSD_60.csv is empty\n",
      "Warning: PROMPTEUR_30.csv is empty\n",
      "Warning: PROMPTUSD_1.csv is empty\n",
      "Warning: SUNEUR_30.csv is empty\n",
      "Warning: HMSTRUSD_240.csv is empty\n",
      "Warning: EDGEEUR_240.csv is empty\n",
      "Warning: USDDUSD_1440.csv is empty\n",
      "Warning: RLUSDUSD_60.csv is empty\n",
      "Warning: XBTUSDR_5.csv is empty\n",
      "Warning: KERNELEUR_1440.csv is empty\n",
      "Warning: SSVUSD_60.csv is empty\n",
      "Warning: ALCHEUR_240.csv is empty\n",
      "Warning: ALCHEUR_15.csv is empty\n",
      "Warning: HMSTRUSD_15.csv is empty\n",
      "Warning: FHEUSD_240.csv is empty\n",
      "Warning: BNBUSDC_30.csv is empty\n",
      "Warning: PROMPTEUR_1.csv is empty\n",
      "Warning: BABYEUR_60.csv is empty\n",
      "Warning: JSTEUR_30.csv is empty\n",
      "Warning: AREUR_30.csv is empty\n",
      "Warning: BNBUSDT_60.csv is empty\n",
      "Warning: RLUSDUSDT_720.csv is empty\n",
      "Warning: ARCEUR_720.csv is empty\n",
      "Warning: GUNUSD_1.csv is empty\n",
      "Warning: XBTUSDR_30.csv is empty\n",
      "Warning: RLUSDUSDT_240.csv is empty\n",
      "Warning: CELOEUR_30.csv is empty\n",
      "Warning: GUNUSD_5.csv is empty\n",
      "Warning: ARCUSD_60.csv is empty\n",
      "Warning: BABYEUR_1440.csv is empty\n",
      "Warning: RLUSDUSDC_30.csv is empty\n",
      "Warning: WINEUR_15.csv is empty\n",
      "Warning: USDDEUR_60.csv is empty\n",
      "Warning: TITCOINEUR_30.csv is empty\n",
      "Warning: ARCEUR_240.csv is empty\n",
      "Warning: WCTEUR_30.csv is empty\n",
      "Warning: PROMPTEUR_5.csv is empty\n",
      "Warning: XBTUSDR_1.csv is empty\n",
      "Warning: FHEUSD_720.csv is empty\n",
      "Warning: USDGEUR_1440.csv is empty\n",
      "Warning: ALCHEUR_720.csv is empty\n",
      "Warning: RLUSDUSDT_60.csv is empty\n",
      "Warning: RLUSDEUR_15.csv is empty\n",
      "Warning: SSVEUR_15.csv is empty\n",
      "Warning: HMSTRUSD_720.csv is empty\n",
      "Warning: BABYUSD_15.csv is empty\n",
      "Warning: EDGEEUR_720.csv is empty\n",
      "Warning: PROMPTUSD_5.csv is empty\n",
      "Warning: ALCHUSD_60.csv is empty\n",
      "Warning: HMSTREUR_1440.csv is empty\n",
      "Warning: HMSTREUR_60.csv is empty\n",
      "Warning: WINEUR_720.csv is empty\n",
      "Warning: GUNEUR_5.csv is empty\n",
      "Warning: GUNUSD_240.csv is empty\n",
      "Warning: ARUSD_1440.csv is empty\n",
      "Warning: RLUSDUSDT_5.csv is empty\n",
      "Warning: BNBEUR_60.csv is empty\n",
      "Warning: TRXUSDD_240.csv is empty\n",
      "Warning: TRXUSDD_60.csv is empty\n",
      "Warning: SSVEUR_240.csv is empty\n",
      "Warning: ALCHUSD_1.csv is empty\n",
      "Warning: PROMPTEUR_240.csv is empty\n",
      "Warning: GUNEUR_15.csv is empty\n",
      "Warning: METISUSD_15.csv is empty\n",
      "Warning: XBTUSDQ_240.csv is empty\n",
      "Warning: TRXUSDD_5.csv is empty\n",
      "Warning: SSVUSD_5.csv is empty\n",
      "Warning: RLUSDEUR_720.csv is empty\n",
      "Warning: FHEUSD_15.csv is empty\n",
      "Warning: FHEEUR_1440.csv is empty\n",
      "Warning: SSVEUR_5.csv is empty\n",
      "Warning: XRPRLUSD_60.csv is empty\n",
      "Warning: KERNELEUR_240.csv is empty\n",
      "Warning: ALCHEUR_1.csv is empty\n",
      "Warning: BNBUSDT_720.csv is empty\n",
      "Warning: WCTUSD_240.csv is empty\n",
      "Warning: BNBEUR_240.csv is empty\n",
      "Warning: TITCOINUSD_720.csv is empty\n",
      "Warning: RLUSDUSDC_240.csv is empty\n",
      "Warning: ARUSD_720.csv is empty\n",
      "Warning: BNBUSDT_15.csv is empty\n",
      "Warning: csv_file_summary.csv is empty\n",
      "Warning: EDGEEUR_1.csv is empty\n",
      "Warning: JSTEUR_240.csv is empty\n",
      "Warning: BABYEUR_15.csv is empty\n",
      "Warning: HMSTRUSD_60.csv is empty\n",
      "Warning: ALCHEUR_60.csv is empty\n",
      "Warning: SSVUSD_15.csv is empty\n",
      "Warning: SUNUSD_5.csv is empty\n",
      "Warning: EDGEEUR_1440.csv is empty\n",
      "Warning: RLUSDUSD_15.csv is empty\n",
      "Warning: SUNUSD_1440.csv is empty\n",
      "Warning: SUNEUR_5.csv is empty\n",
      "Warning: WCTUSD_30.csv is empty\n",
      "Warning: SUNUSD_720.csv is empty\n",
      "Warning: BNBUSDT_1.csv is empty\n",
      "Warning: WINUSD_15.csv is empty\n",
      "Warning: USDDUSD_60.csv is empty\n",
      "Warning: XBTUSDQ_15.csv is empty\n",
      "Warning: TITCOINUSD_30.csv is empty\n",
      "Warning: EDGEUSD_1.csv is empty\n",
      "Warning: CELOUSD_30.csv is empty\n",
      "Warning: USDGEUR_30.csv is empty\n",
      "Warning: ARCEUR_60.csv is empty\n",
      "Warning: WINUSD_1.csv is empty\n",
      "Warning: RLUSDUSDC_1440.csv is empty\n",
      "Warning: FHEUSD_1.csv is empty\n",
      "Warning: XRPRLUSD_240.csv is empty\n",
      "Warning: ARUSD_5.csv is empty\n",
      "Warning: XBTUSDQ_5.csv is empty\n",
      "Warning: USDDEUR_1.csv is empty\n",
      "Warning: CELOEUR_240.csv is empty\n",
      "Warning: USDGEUR_240.csv is empty\n",
      "Warning: FHEEUR_15.csv is empty\n",
      "Warning: METISUSD_1.csv is empty\n",
      "Warning: USDDUSD_720.csv is empty\n",
      "Warning: GUNUSD_15.csv is empty\n",
      "Warning: METISEUR_15.csv is empty\n",
      "Warning: BABYUSD_720.csv is empty\n",
      "Warning: METISEUR_1.csv is empty\n",
      "Warning: KERNELUSD_1440.csv is empty\n",
      "Warning: USDDUSD_1.csv is empty\n",
      "Warning: USDDEUR_1440.csv is empty\n",
      "Warning: METISEUR_720.csv is empty\n",
      "Warning: BNBUSDC_720.csv is empty\n",
      "Warning: AREUR_5.csv is empty\n",
      "Warning: BNBUSD_60.csv is empty\n",
      "Warning: XBTUSDR_720.csv is empty\n",
      "Warning: WINEUR_1.csv is empty\n",
      "Warning: FHEEUR_1.csv is empty\n",
      "Warning: FHEEUR_5.csv is empty\n",
      "Warning: XBTUSDR_240.csv is empty\n",
      "Warning: WINEUR_5.csv is empty\n",
      "Warning: METISEUR_240.csv is empty\n",
      "Warning: BNBUSDC_240.csv is empty\n",
      "Warning: BABYUSD_1440.csv is empty\n",
      "Warning: AREUR_1.csv is empty\n",
      "Warning: BABYUSD_240.csv is empty\n",
      "Warning: ETHAED_240.csv is empty\n",
      "Warning: USDDUSD_240.csv is empty\n",
      "Warning: XRPRLUSD_15.csv is empty\n",
      "Warning: USDDUSD_5.csv is empty\n",
      "Warning: METISEUR_5.csv is empty\n",
      "Warning: FHEUSD_60.csv is empty\n",
      "Warning: HMSTRUSD_1440.csv is empty\n",
      "Warning: METISUSD_5.csv is empty\n",
      "Warning: USDGEUR_720.csv is empty\n",
      "Warning: CELOEUR_720.csv is empty\n",
      "Warning: METISUSD_60.csv is empty\n",
      "Warning: GUNEUR_60.csv is empty\n",
      "Warning: KERNELEUR_30.csv is empty\n",
      "Warning: USDDEUR_5.csv is empty\n",
      "Warning: TRXUSDD_15.csv is empty\n",
      "Warning: XRPRLUSD_720.csv is empty\n",
      "Warning: XBTUSDQ_1.csv is empty\n",
      "Warning: ARUSD_1.csv is empty\n",
      "Warning: BNBEUR_15.csv is empty\n",
      "Warning: EDGEUSD_30.csv is empty\n",
      "Warning: FHEUSD_5.csv is empty\n",
      "Warning: WINUSD_5.csv is empty\n",
      "Warning: KUSD_720.csv is empty\n",
      "Warning: AREUR_1440.csv is empty\n",
      "Warning: EDGEUSD_5.csv is empty\n",
      "Warning: JSTUSD_30.csv is empty\n",
      "Warning: ARUSD_30.csv is empty\n",
      "Warning: HMSTREUR_15.csv is empty\n",
      "Warning: BNBUSDT_5.csv is empty\n",
      "Warning: ALCHUSD_15.csv is empty\n",
      "Warning: BABYUSD_60.csv is empty\n",
      "Warning: SUNUSD_240.csv is empty\n",
      "Warning: XBTUSDQ_1440.csv is empty\n",
      "Warning: SUNEUR_1.csv is empty\n",
      "Warning: SSVEUR_60.csv is empty\n",
      "Warning: RLUSDEUR_60.csv is empty\n",
      "Warning: RLUSDUSDT_15.csv is empty\n",
      "Warning: SUNUSD_1.csv is empty\n",
      "Warning: FHEUSD_1440.csv is empty\n",
      "Warning: PROMPTUSD_30.csv is empty\n",
      "Warning: SUNUSD_30.csv is empty\n",
      "Warning: USDDEUR_15.csv is empty\n",
      "Warning: WINEUR_60.csv is empty\n",
      "Warning: EDGEEUR_5.csv is empty\n",
      "Warning: ARUSD_240.csv is empty\n",
      "Warning: RLUSDUSDC_720.csv is empty\n",
      "Warning: ARCUSD_15.csv is empty\n",
      "Warning: JSTEUR_720.csv is empty\n",
      "Warning: BNBUSDT_1440.csv is empty\n",
      "\n",
      "Summary of CSV files:\n",
      "             file_name  first_unix_time  last_unix_time           first_date  \\\n",
      "0         TRUUSD_1.csv       1661529540      1743460500  2022-08-26 15:59:00   \n",
      "1       MEEUR_1440.csv       1738022400      1743033600  2025-01-28 00:00:00   \n",
      "2        ARPAUSD_5.csv       1659018900      1743452400  2022-07-28 14:35:00   \n",
      "3      OCEANGBP_30.csv       1704067200      1727712000  2024-01-01 00:00:00   \n",
      "4      LAYERUSD_30.csv       1741116600      1743462000  2025-03-04 19:30:00   \n",
      "...                ...              ...             ...                  ...   \n",
      "8233  ATLASUSD_720.csv       1643284800      1743422400  2022-01-27 12:00:00   \n",
      "8234     INJUSD_60.csv       1628607600      1743462000  2021-08-10 15:00:00   \n",
      "8235   NTRNUSD_240.csv       1721145600      1743451200  2024-07-16 16:00:00   \n",
      "8236   STGUSD_1440.csv       1664409600      1743379200  2022-09-29 00:00:00   \n",
      "8237  USDQUSD_1440.csv       1732147200      1743379200  2024-11-21 00:00:00   \n",
      "\n",
      "                last_date  number_rows  \n",
      "0     2025-03-31 22:35:00        83869  \n",
      "1     2025-03-27 00:00:00           49  \n",
      "2     2025-03-31 20:20:00        32100  \n",
      "3     2024-09-30 16:00:00         5209  \n",
      "4     2025-03-31 23:00:00          691  \n",
      "...                   ...          ...  \n",
      "8233  2025-03-31 12:00:00         2318  \n",
      "8234  2025-03-31 23:00:00        29704  \n",
      "8235  2025-03-31 20:00:00         1427  \n",
      "8236  2025-03-31 00:00:00          914  \n",
      "8237  2025-03-31 00:00:00          108  \n",
      "\n",
      "[8238 rows x 6 columns]\n",
      "\n",
      "Summary saved to /Users/xin/Stanford/MSE 349 Financial Statistics/crypto_data/Kraken_OHLCVT/csv_file_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Specify the folder containing the CSV files\n",
    "folder_path = '/Users/xin/Stanford/MSE 349 Financial Statistics/crypto_data/Kraken_OHLCVT'\n",
    "\n",
    "# Expected columns in the CSV files\n",
    "expected_columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'trades']\n",
    "expected_num_columns = len(expected_columns)\n",
    "\n",
    "# List to store information about each file\n",
    "file_info_list = []\n",
    "\n",
    "# Iterate over all files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        # Print empty files and exclude them\n",
    "        if is_empty(file_path):\n",
    "            print(f\"Warning: {file_name} is empty\")\n",
    "            continue\n",
    "\n",
    "        # Read the CSV file, with or without header\n",
    "        if has_header(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "        else:\n",
    "            df = pd.read_csv(file_path, header=None, names=expected_columns)\n",
    "        \n",
    "        # 1. Check if the file has the expected 7 columns\n",
    "        if len(df.columns) != expected_num_columns or not all(col in expected_columns for col in df.columns):\n",
    "            print(f\"Warning: {file_name} does not have the expected {expected_num_columns} columns: {df.columns.tolist()}\")\n",
    "            continue\n",
    "        \n",
    "        # 2. Extract first and last timestamps\n",
    "        first_unix_time = df['timestamp'].iloc[0]\n",
    "        last_unix_time = df['timestamp'].iloc[-1]\n",
    "        \n",
    "        # Convert timestamps to human-readable UTC dates\n",
    "        # Use timezone.utc instead of UTC\n",
    "        first_date = datetime.fromtimestamp(first_unix_time, timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        last_date = datetime.fromtimestamp(last_unix_time, timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        number_rows = len(df)\n",
    "        \n",
    "        # Store file information\n",
    "        file_info = {\n",
    "            'file_name': file_name,\n",
    "            'first_unix_time': first_unix_time,\n",
    "            'last_unix_time': last_unix_time,\n",
    "            'first_date': first_date,\n",
    "            'last_date': last_date,\n",
    "            'number_rows': number_rows\n",
    "        }\n",
    "        file_info_list.append(file_info)\n",
    "\n",
    "\n",
    "# Convert file info list to a DataFrame for easy viewing\n",
    "file_info_df = pd.DataFrame(file_info_list)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nSummary of CSV files:\")\n",
    "print(file_info_df)\n",
    "\n",
    "# Optionally, save the summary to a CSV file\n",
    "output_path = os.path.join(folder_path, 'csv_file_summary.csv')\n",
    "file_info_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nSummary saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         file_name  first_unix_time  last_unix_time           first_date            last_date  number_rows\n",
      "0  XBTEUR_1440.csv       1378771200      1743379200  2013-09-10 00:00:00  2025-03-31 00:00:00         4216\n",
      "1   XBTEUR_720.csv       1378814400      1743422400  2013-09-10 12:00:00  2025-03-31 12:00:00         8412\n",
      "2    XBTEUR_60.csv       1378854000      1743462000  2013-09-10 23:00:00  2025-03-31 23:00:00       100032\n",
      "3    XBTEUR_15.csv       1378856700      1743464700  2013-09-10 23:45:00  2025-03-31 23:45:00       395562\n",
      "4     XBTEUR_5.csv       1378856700      1743465300  2013-09-10 23:45:00  2025-03-31 23:55:00      1152434\n",
      "5     XBTEUR_1.csv       1378856820      1743465540  2013-09-10 23:47:00  2025-03-31 23:59:00      5172692\n",
      "6  LTCEUR_1440.csv       1379116800      1743379200  2013-09-14 00:00:00  2025-03-31 00:00:00         4177\n",
      "7   LTCEUR_720.csv       1379116800      1743422400  2013-09-14 00:00:00  2025-03-31 12:00:00         8336\n",
      "8    LTCEUR_60.csv       1379145600      1743462000  2013-09-14 08:00:00  2025-03-31 23:00:00        90811\n",
      "9    LTCEUR_15.csv       1379146500      1743464700  2013-09-14 08:15:00  2025-03-31 23:45:00       321776\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)  # Don't truncate column content\n",
    "pd.set_option('display.width', 1000)        # Set a large width to fit content on one line\n",
    "\n",
    "filepath = '/Users/xin/Stanford/MSE 349 Financial Statistics/crypto_data/Kraken_OHLCVT_summary/csv_file_summary.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "print(df.head(10))\n",
    "# df = pd.read_csv(filepath)\n",
    "# df.sort_values(by='first_unix_time', ascending=True, inplace=True)\n",
    "# df.to_csv(filepath, index=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with unrecognized or missing intervals:\n",
      "                file_name  first_unix_time  last_unix_time           first_date            last_date  number_rows interval\n",
      "12  BTCUSD_Daily_OHLC.csv       1381017600      1703980800  2013-10-06 00:00:00  2023-12-31 00:00:00         3727     None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the original file\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# Define allowed intervals\n",
    "valid_intervals = {\"1\", \"5\", \"15\", \"30\", \"60\", \"240\", \"720\", \"1440\"}\n",
    "\n",
    "# Function to extract interva (timestep) from filename\n",
    "def extract_interval(filename):\n",
    "    match = re.search(r'_(\\d+)\\.csv$', filename)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Apply extraction\n",
    "df['interval'] = df.iloc[:, 0].apply(extract_interval)\n",
    "\n",
    "# Split the summary df by interval, create a new summary df for each interval\n",
    "for interval in valid_intervals:\n",
    "    df_interval = df[df['interval'] == interval]\n",
    "    if not df_interval.empty:\n",
    "        df_interval.drop(columns='interval').to_csv(f\"{filepath}_{interval}.csv\", index=False)\n",
    "\n",
    "# Identify and print rows with invalid or missing intervals\n",
    "invalid_rows = df[~df['interval'].isin(valid_intervals)]\n",
    "if not invalid_rows.empty:\n",
    "    print(\"Rows with unrecognized or missing intervals:\")\n",
    "    print(invalid_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of file 1: 1038\n",
      "Length of file 5: 1037\n",
      "Length of file 15: 1037\n",
      "Length of file 30: 1009\n",
      "Length of file 60: 1037\n",
      "Length of file 240: 1009\n",
      "Length of file 720: 1035\n",
      "Length of file 1440: 1035\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Define your base filepath (without _<interval>.csv)\n",
    "base_filepath = filepath  # e.g., \"data/XBTEUR\"\n",
    "\n",
    "# Define valid intervals\n",
    "intervals = [\"1\", \"5\", \"15\", \"30\", \"60\", \"240\", \"720\", \"1440\"]\n",
    "\n",
    "for nb in intervals:\n",
    "    file = f\"{base_filepath}_{nb}.csv\"\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        df.sort_values(by='first_unix_time', ascending=True, inplace=True)\n",
    "        df.to_csv(file, index=False)\n",
    "        print(f\"Length of file {nb}: {len(df)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{file} not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Option: HDF5 (Hierarchical Data Format) using pandas.HDFStore\n",
    "\n",
    "You can save each interval as a separate key.\n",
    "\n",
    "Data is stored efficiently, even for large DataFrames.\n",
    "\n",
    "You can read only one interval at a time without loading everything.\n",
    "\n",
    "Sorting can be preserved on save.\n",
    "\n",
    "CSV can’t store multiple DataFrames in one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# !pip install tables\n",
    "# Define your base filepath (without _<interval>.csv)\n",
    "intervals = [\"1\", \"5\", \"15\", \"30\", \"60\", \"240\", \"720\", \"1440\"]\n",
    "\n",
    "# Save all sorted DataFrames into a single HDF5 file\n",
    "store_path = f\"{base_filepath}_all_intervals.h5\"\n",
    "\n",
    "with pd.HDFStore(store_path, mode='w') as store:\n",
    "    for interval in intervals:\n",
    "        csv_path = f\"{filepath}_{interval}.csv\"\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df.sort_values(by='first_unix_time', ascending=True, inplace=True)\n",
    "        store.put(f\"{interval}\", df, format='table')\n",
    "\n",
    "print(f\"All intervals saved to {store_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       file_name  first_unix_time  last_unix_time           first_date            last_date  number_rows\n",
      "0  XBTEUR_60.csv       1378854000      1743462000  2013-09-10 23:00:00  2025-03-31 23:00:00       100032\n",
      "1  LTCEUR_60.csv       1379145600      1743462000  2013-09-14 08:00:00  2025-03-31 23:00:00        90811\n",
      "2  XBTUSD_60.csv       1381093200      1743462000  2013-10-06 21:00:00  2025-03-31 23:00:00        89787\n",
      "3  LTCUSD_60.csv       1382619600      1743462000  2013-10-24 13:00:00  2025-03-31 23:00:00        77970\n",
      "4  XBTJPY_60.csv       1415224800      1743462000  2014-11-05 22:00:00  2025-03-31 23:00:00        48947\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Open the HDF5 file\n",
    "store_path = f\"{filepath}_all_intervals.h5\"\n",
    "\n",
    "with pd.HDFStore(store_path, mode='r') as store:\n",
    "    df60 = store[\"60\"]  # for 15-minute interval\n",
    "    print(df60.head())           # use the DataFrame as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>first_unix_time</th>\n",
       "      <th>last_unix_time</th>\n",
       "      <th>first_date</th>\n",
       "      <th>last_date</th>\n",
       "      <th>number_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>KEUR_60.csv</td>\n",
       "      <td>1743436800</td>\n",
       "      <td>1743454800</td>\n",
       "      <td>2025-03-31 16:00:00</td>\n",
       "      <td>2025-03-31 21:00:00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>KUSD_60.csv</td>\n",
       "      <td>1743433200</td>\n",
       "      <td>1743462000</td>\n",
       "      <td>2025-03-31 15:00:00</td>\n",
       "      <td>2025-03-31 23:00:00</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>CORNUSD_60.csv</td>\n",
       "      <td>1743156000</td>\n",
       "      <td>1743462000</td>\n",
       "      <td>2025-03-28 10:00:00</td>\n",
       "      <td>2025-03-31 23:00:00</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>CORNEUR_60.csv</td>\n",
       "      <td>1743156000</td>\n",
       "      <td>1743462000</td>\n",
       "      <td>2025-03-28 10:00:00</td>\n",
       "      <td>2025-03-31 23:00:00</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>GHIBLIEUR_60.csv</td>\n",
       "      <td>1743105600</td>\n",
       "      <td>1743451200</td>\n",
       "      <td>2025-03-27 20:00:00</td>\n",
       "      <td>2025-03-31 20:00:00</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             file_name  first_unix_time  last_unix_time           first_date            last_date  number_rows\n",
       "1036       KEUR_60.csv       1743436800      1743454800  2025-03-31 16:00:00  2025-03-31 21:00:00            3\n",
       "1035       KUSD_60.csv       1743433200      1743462000  2025-03-31 15:00:00  2025-03-31 23:00:00            9\n",
       "1034    CORNUSD_60.csv       1743156000      1743462000  2025-03-28 10:00:00  2025-03-31 23:00:00           80\n",
       "1033    CORNEUR_60.csv       1743156000      1743462000  2025-03-28 10:00:00  2025-03-31 23:00:00           69\n",
       "1032  GHIBLIEUR_60.csv       1743105600      1743451200  2025-03-27 20:00:00  2025-03-31 20:00:00           32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df60.sort_values(by='first_unix_time', ascending=False, inplace=True)\n",
    "df60.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_by_time(df, start_day=None, end_day=None, return_df=False):\n",
    "    \"\"\"\n",
    "    Filter rows in `df` where:\n",
    "    - `first_date` is on or before `start_day` (if provided)\n",
    "    - `last_date` is on or after `end_day` (if provided)\n",
    "\n",
    "    If time is not provided in dates, defaults to 00:00:00.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with 'first_date' and 'last_date' columns (as datetime or string)\n",
    "        start_day (str): e.g. \"2025-03-27\" or \"2025-03-27 14:30:00\"\n",
    "        end_day (str): e.g. \"2025-03-31\" or \"2025-03-31 23:59:59\"\n",
    "        return_df (bool): Whether to return the filtered DataFrame\n",
    "\n",
    "    Returns:\n",
    "        int: number of matching rows\n",
    "        pd.DataFrame (optional): filtered DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    df = df.copy()\n",
    "    df['first_date'] = pd.to_datetime(df['first_date'])\n",
    "    df['last_date'] = pd.to_datetime(df['last_date'])\n",
    "\n",
    "    # Convert inputs to datetime objects, defaulting to 00:00:00 if time not provided\n",
    "    if start_day:\n",
    "        start_day = pd.to_datetime(start_day)\n",
    "        if start_day.hour == 0 and start_day.minute == 0 and start_day.second == 0:\n",
    "            start_day = start_day.replace(hour=0, minute=0, second=0)\n",
    "    if end_day:\n",
    "        end_day = pd.to_datetime(end_day)\n",
    "        if end_day.hour == 0 and end_day.minute == 0 and end_day.second == 0:\n",
    "            end_day = end_day.replace(hour=0, minute=0, second=0)\n",
    "\n",
    "    # Apply filters\n",
    "    if start_day and end_day:\n",
    "        mask = (df['first_date'] <= start_day) & (df['last_date'] >= end_day)\n",
    "    elif start_day:\n",
    "        mask = df['first_date'] <= start_day\n",
    "    elif end_day:\n",
    "        mask = df['last_date'] >= end_day\n",
    "    else:\n",
    "        mask = pd.Series([True] * len(df))  # No filtering\n",
    "\n",
    "    filtered_df = df[mask]\n",
    "    \n",
    "    if return_df:\n",
    "        return len(filtered_df), filtered_df\n",
    "    else:\n",
    "        return len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets available from 2020.1.1-2025.3.31:  93\n",
      "datasets available from 2022.1.1-2025.3.31:  305\n"
     ]
    }
   ],
   "source": [
    "d20 = filter_by_time(df60, start_day=\"2020-01-01\", end_day=\"2025-03-31\")\n",
    "d22 = filter_by_time(df60, start_day=\"2022-01-01\", end_day=\"2025-03-31\")\n",
    "print(\"datasets available from 2020.1.1-2025.3.31: \", d20)\n",
    "print(\"datasets available from 2022.1.1-2025.3.31: \", d22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of filtered dataframe:  305\n",
      "Expected number of rows: 28440 hours\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def compute_hours(start_day, end_day):\n",
    "    \"\"\" \n",
    "    Calculate the number of hours elapsed between two dates.\n",
    "    \"\"\"\n",
    "    start = pd.to_datetime(start_day)\n",
    "    end = pd.to_datetime(end_day)\n",
    "    hours = (end - start).total_seconds() / 3600\n",
    "    return int(hours)\n",
    "\n",
    "\n",
    "def compute_entries_in_range(data_input, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Count rows in dataset within the given date range.\n",
    "    Args:\n",
    "        data_input: Either a path to CSV file or a pandas DataFrame\n",
    "        start_date: Start date in 'YYYY-MM-DD' or 'YYYY-MM-DD HH:MM:SS' format\n",
    "        end_date: End date in 'YYYY-MM-DD' or 'YYYY-MM-DD HH:MM:SS' format\n",
    "        (assume time is 00:00:00 if not given)\n",
    "    Returns:\n",
    "        int: Number of rows in range, or False if range is invalid\n",
    "    \"\"\"\n",
    "    start_dt = pd.to_datetime(start_date).replace(hour=0, minute=0, second=0, microsecond=0) if len(start_date) <= 10 else pd.to_datetime(start_date)\n",
    "    end_dt = pd.to_datetime(end_date).replace(hour=0, minute=0, second=0, microsecond=0) if len(end_date) <= 10 else pd.to_datetime(end_date)\n",
    "    start_unix, end_unix = start_dt.timestamp(), end_dt.timestamp()\n",
    "\n",
    "    # Handle input data\n",
    "    df = pd.read_csv(data_input) if isinstance(data_input, str) else data_input\n",
    "    \n",
    "    if df.iloc[:, 0].min() > start_unix or df.iloc[:, 0].max() < end_unix:\n",
    "        return False\n",
    "        \n",
    "    # Count rows in range (inclusive)\n",
    "    mask = (df.iloc[:, 0] >= start_unix) & (df.iloc[:, 0] <= end_unix)\n",
    "    return len(df[mask])\n",
    "\n",
    "\n",
    "# Get filtered dataframe and sort by number_rows\n",
    "start_day = \"2022-01-01\"\n",
    "end_day = \"2025-03-31\"\n",
    "\n",
    "filtered_len, filtered_df = filter_by_time(df60, start_day=start_day, end_day=end_day, return_df=True)\n",
    "print(\"Length of filtered dataframe: \", filtered_len)\n",
    "# Test the function with the date range from the previous example\n",
    "expected_rows = compute_hours(start_day, end_day)\n",
    "print(f\"Expected number of rows: {expected_rows} hours\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def filter_by_suffix(name, df, return_df=False):\n",
    "    \"\"\"\n",
    "    Filter rows where file_name contains the currency \"name\" right before the underscore.\n",
    "    e.g. name=\"USD\" then return files with names like \"BTCUSD_60.csv\" but not \"USDEUR_60.csv\"\n",
    "\n",
    "    Args:\n",
    "        name (str): The currency name to search for (e.g., 'USD', 'USDT')\n",
    "        df (pd.DataFrame): The dataframe to filter, with file_name column\n",
    "        return_df (bool): If True, return filtered dataframe along with count\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (count, filtered_df) if return_df=True, else just count\n",
    "    \"\"\"\n",
    "    # Create pattern that matches prefix right before underscore\n",
    "    pattern = f\"{name}_\"\n",
    "    \n",
    "    # Filter rows where file_name contains the pattern\n",
    "    mask = df['file_name'].str.contains(pattern, regex=False)\n",
    "    filtered_df = df[mask]\n",
    "    \n",
    "    if return_df:\n",
    "        return len(filtered_df), filtered_df\n",
    "    return len(filtered_df)\n",
    "\n",
    "# Test the function\n",
    "# test_prefixes = ['USD', 'USDT', 'EUR']\n",
    "# for prefix in test_prefixes:\n",
    "#     count, filtered = filter_by_suffix(prefix, filtered_df, return_df=True)\n",
    "#     print(f\"\\nPrefix '{prefix}':\")\n",
    "#     print(f\"Found {count} matching files\")\n",
    "#     print(\"First few matches:\")\n",
    "#     print(filtered['file_name'].head())\n",
    "\n",
    "def find_missing_entries(csv_path, start_day, end_day, interval='60'):\n",
    "    \"\"\"\n",
    "    Find missing entries in a time series dataset based on expected timesteps.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file\n",
    "        start_day (str): Start date in format \"YYYY-MM-DD\" or \"YYYY-MM-DD HH:MM:SS\"\n",
    "        end_day (str): End date in format \"YYYY-MM-DD\" or \"YYYY-MM-DD HH:MM:SS\"\n",
    "        interval (str): Time interval in minutes (default '60')\n",
    "        \n",
    "    Returns:\n",
    "        List of missing entry indices (0-based)\n",
    "    \"\"\"\n",
    "    # Convert interval to seconds\n",
    "    timestep = int(interval) * 60\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    \n",
    "    # Convert start and end days to unix timestamps\n",
    "    start_unix = pd.to_datetime(start_day).timestamp()\n",
    "    end_unix = pd.to_datetime(end_day).timestamp()\n",
    "    \n",
    "    # Filter data within date range\n",
    "    mask = (df[0] >= start_unix) & (df[0] <= end_unix)\n",
    "    df = df[mask]\n",
    "    \n",
    "    # Convert timestamps to indices (0-based) first\n",
    "    actual_entries = ((df[0] - start_unix) // timestep).astype(int)\n",
    "    \n",
    "    # Calculate expected entries and create set of expected indices\n",
    "    expected_entries = int((end_unix - start_unix)/timestep) + 1\n",
    "    # Find missing indices\n",
    "    missing_indices = set(range(expected_entries)) - set(actual_entries)\n",
    "    \n",
    "    return sorted(missing_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390\n",
      "            file_name  first_unix_time  last_unix_time           first_date            last_date  number_rows\n",
      "380   LSETHUSD_60.csv       1742223600      1743264000  2025-03-17 15:00:00  2025-03-29 16:00:00          121\n",
      "381    DUCKUSD_60.csv       1742396400      1743462000  2025-03-19 15:00:00  2025-03-31 23:00:00          225\n",
      "382  APENFTUSD_60.csv       1742482800      1743462000  2025-03-20 15:00:00  2025-03-31 23:00:00          271\n",
      "383   SPICEUSD_60.csv       1742482800      1743462000  2025-03-20 15:00:00  2025-03-31 23:00:00          231\n",
      "384     NILUSD_60.csv       1742997600      1743462000  2025-03-26 14:00:00  2025-03-31 23:00:00          123\n",
      "385     WALUSD_60.csv       1743069600      1743462000  2025-03-27 10:00:00  2025-03-31 23:00:00          110\n",
      "386    TERMUSD_60.csv       1743076800      1743447600  2025-03-27 12:00:00  2025-03-31 19:00:00           37\n",
      "387  GHIBLIUSD_60.csv       1743098400      1743462000  2025-03-27 18:00:00  2025-03-31 23:00:00          102\n",
      "388    CORNUSD_60.csv       1743156000      1743462000  2025-03-28 10:00:00  2025-03-31 23:00:00           80\n",
      "389       KUSD_60.csv       1743433200      1743462000  2025-03-31 15:00:00  2025-03-31 23:00:00            9\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.width', 1000)\n",
    "csv = 'Kraken_OHLCVT_summary/csv_file_summary.csv_60.csv'\n",
    "summary = pd.read_csv(csv)\n",
    "num, dfUSD = filter_by_suffix(\"USD\", summary, return_df=True)\n",
    "print(num)\n",
    "dfUSD = dfUSD.reset_index(drop=True)\n",
    "print(dfUSD.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "N, df_USD = filter_by_suffix(\"USD\", filtered_df, return_df=True)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 21, 22, 24, 25, 26, 28, 31, 32, 34, 36, 37, 38, 39, 40, 41, 42, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 64, 66, 67, 72, 74, 75, 76, 78, 79, 80, 81, 84, 85, 88, 89, 96, 98, 100, 101, 103, 104, 105, 106, 107, 108, 110, 111, 112, 113, 114, 118, 119, 120, 121, 130, 131, 132, 133, 134, 135, 137, 138, 139, 143, 144, 145, 146, 149, 151, 152, 154, 155, 156]\n",
      "21686\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "sample_file = f\"Kraken_OHLCVT/{df_USD.iloc[k]['file_name']}\"\n",
    "list = find_missing_entries(sample_file, start_day, end_day)\n",
    "print(list[:100])\n",
    "print(len(list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with saved `filtered_df` \n",
    "(60min data from 2020-01-01 00:00:00 to 2-25-03-31 00:00:00, with 305 tokens available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a list to store results\n",
    "results = []\n",
    "#missing = [] # a list of lists containing missing entries for each file\n",
    "\n",
    "# Calculate entries for each file\n",
    "for k in range(N):\n",
    "    sample_file = f\"Kraken_OHLCVT/{df_USD.iloc[k]['file_name']}\"\n",
    "    #missing.append(find_missing_entries(sample_file, start_day, end_day))\n",
    "    results.append(compute_entries_in_range(sample_file, start_day, end_day))\n",
    "\n",
    "# Convert to numpy array and get sorted indices\n",
    "results_array = np.array(results)\n",
    "sorted_indices = np.argsort(results_array)\n",
    "\n",
    "# Sort both results and filtered_df using the same indices\n",
    "sorted_results = results_array[sorted_indices]\n",
    "df_USD = df_USD.iloc[sorted_indices].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 TBTCUSD_60.csv      : 6755/28440 existing entries\n",
      "   2 BNTUSD_60.csv       : 13585/28440 existing entries\n",
      "   3 REPUSD_60.csv       : 13649/28440 existing entries\n",
      "   4 GHSTUSD_60.csv      : 14132/28440 existing entries\n",
      "   5 SDNUSD_60.csv       : 14766/28440 existing entries\n",
      "   6 BADGERUSD_60.csv    : 15864/28440 existing entries\n",
      "   7 PERPUSD_60.csv      : 16434/28440 existing entries\n",
      "   8 OXYUSD_60.csv       : 16505/28440 existing entries\n",
      "   9 WBTCUSD_60.csv      : 16658/28440 existing entries\n",
      "  10 BALUSD_60.csv       : 17054/28440 existing entries\n",
      "  11 RAYUSD_60.csv       : 18940/28440 existing entries\n",
      "  12 CTSIUSD_60.csv      : 19178/28440 existing entries\n",
      "  13 KARUSD_60.csv       : 19403/28440 existing entries\n",
      "  14 BNCUSD_60.csv       : 19565/28440 existing entries\n",
      "  15 BANDUSD_60.csv      : 19661/28440 existing entries\n",
      "  16 KEEPUSD_60.csv      : 20161/28440 existing entries\n",
      "  17 OGNUSD_60.csv       : 20236/28440 existing entries\n",
      "  18 LSKUSD_60.csv       : 20367/28440 existing entries\n",
      "  19 REPV2USD_60.csv     : 20397/28440 existing entries\n",
      "  20 GNOUSD_60.csv       : 20461/28440 existing entries\n",
      "  21 KNCUSD_60.csv       : 20561/28440 existing entries\n",
      "  22 MLNUSD_60.csv       : 21390/28440 existing entries\n",
      "  23 ICXUSD_60.csv       : 21607/28440 existing entries\n",
      "  24 RARIUSD_60.csv      : 21608/28440 existing entries\n",
      "  25 YFIUSD_60.csv       : 21848/28440 existing entries\n",
      "  26 MIRUSD_60.csv       : 21926/28440 existing entries\n",
      "  27 LPTUSD_60.csv       : 22031/28440 existing entries\n",
      "  28 CQTUSD_60.csv       : 22079/28440 existing entries\n",
      "  29 KILTUSD_60.csv      : 22132/28440 existing entries\n",
      "  30 RENUSD_60.csv       : 22143/28440 existing entries\n",
      "  31 QTUMUSD_60.csv      : 22490/28440 existing entries\n",
      "  32 OXTUSD_60.csv       : 23057/28440 existing entries\n",
      "  33 SRMUSD_60.csv       : 23063/28440 existing entries\n",
      "  34 ZRXUSD_60.csv       : 23904/28440 existing entries\n",
      "  35 PHAUSD_60.csv       : 23948/28440 existing entries\n",
      "  36 STORJUSD_60.csv     : 24328/28440 existing entries\n",
      "  37 ANKRUSD_60.csv      : 24726/28440 existing entries\n",
      "  38 OMGUSD_60.csv       : 24822/28440 existing entries\n",
      "  39 MOVRUSD_60.csv      : 25246/28440 existing entries\n",
      "  40 LRCUSD_60.csv       : 25273/28440 existing entries\n",
      "  41 SUSHIUSD_60.csv     : 25291/28440 existing entries\n",
      "  42 COMPUSD_60.csv      : 25303/28440 existing entries\n",
      "  43 BATUSD_60.csv       : 25338/28440 existing entries\n",
      "  44 1INCHUSD_60.csv     : 25351/28440 existing entries\n",
      "  45 DYDXUSD_60.csv      : 25770/28440 existing entries\n",
      "  46 CHZUSD_60.csv       : 26111/28440 existing entries\n",
      "  47 MKRUSD_60.csv       : 26124/28440 existing entries\n",
      "  48 DASHUSD_60.csv      : 26486/28440 existing entries\n",
      "  49 AXSUSD_60.csv       : 26543/28440 existing entries\n",
      "  50 INJUSD_60.csv       : 26575/28440 existing entries\n",
      "  51 NANOUSD_60.csv      : 26633/28440 existing entries\n",
      "  52 PAXGUSD_60.csv      : 26737/28440 existing entries\n",
      "  53 ENJUSD_60.csv       : 26922/28440 existing entries\n",
      "  54 ETCUSD_60.csv       : 27062/28440 existing entries\n",
      "  55 OCEANUSD_60.csv     : 27180/28440 existing entries\n",
      "  56 ZECUSD_60.csv       : 27276/28440 existing entries\n",
      "  57 SNXUSD_60.csv       : 27367/28440 existing entries\n",
      "  58 CRVUSD_60.csv       : 27605/28440 existing entries\n",
      "  59 EWTUSD_60.csv       : 27638/28440 existing entries\n",
      "  60 SCUSD_60.csv        : 27686/28440 existing entries\n",
      "  61 KAVAUSD_60.csv      : 27688/28440 existing entries\n",
      "  62 DAIUSD_60.csv       : 27790/28440 existing entries\n",
      "  63 SANDUSD_60.csv      : 27849/28440 existing entries\n",
      "  64 MANAUSD_60.csv      : 27886/28440 existing entries\n",
      "  65 MINAUSD_60.csv      : 27963/28440 existing entries\n",
      "  66 XTZUSD_60.csv       : 28000/28440 existing entries\n",
      "  67 EOSUSD_60.csv       : 28009/28440 existing entries\n",
      "  68 FLOWUSD_60.csv      : 28015/28440 existing entries\n",
      "  69 AUDUSD_60.csv       : 28186/28440 existing entries\n",
      "  70 GRTUSD_60.csv       : 28237/28440 existing entries\n",
      "  71 AAVEUSD_60.csv      : 28252/28440 existing entries\n",
      "  72 FILUSD_60.csv       : 28279/28440 existing entries\n",
      "  73 KSMUSD_60.csv       : 28316/28440 existing entries\n",
      "  74 UNIUSD_60.csv       : 28317/28440 existing entries\n",
      "  75 BCHUSD_60.csv       : 28334/28440 existing entries\n",
      "  76 XRPUSD_60.csv       : 28373/28440 existing entries\n",
      "  77 LUNAUSD_60.csv      : 28409/28440 existing entries\n",
      "  78 ALGOUSD_60.csv      : 28413/28440 existing entries\n",
      "  79 AVAXUSD_60.csv      : 28420/28440 existing entries\n",
      "  80 XLMUSD_60.csv       : 28420/28440 existing entries\n",
      "  81 SHIBUSD_60.csv      : 28422/28440 existing entries\n",
      "  82 TRXUSD_60.csv       : 28422/28440 existing entries\n",
      "  83 ADAUSD_60.csv       : 28423/28440 existing entries\n",
      "  84 LINKUSD_60.csv      : 28423/28440 existing entries\n",
      "  85 ATOMUSD_60.csv      : 28423/28440 existing entries\n",
      "  86 USDCUSD_60.csv      : 28423/28440 existing entries\n",
      "  87 LTCUSD_60.csv       : 28424/28440 existing entries\n",
      "  88 XMRUSD_60.csv       : 28424/28440 existing entries\n",
      "  89 USDTUSD_60.csv      : 28424/28440 existing entries\n",
      "  90 MATICUSD_60.csv     : 28424/28440 existing entries\n",
      "  91 EURUSD_60.csv       : 28424/28440 existing entries\n",
      "  92 GBPUSD_60.csv       : 28424/28440 existing entries\n",
      "  93 DOTUSD_60.csv       : 28424/28440 existing entries\n",
      "  94 SOLUSD_60.csv       : 28424/28440 existing entries\n",
      "  95 ETHUSD_60.csv       : 28425/28440 existing entries\n",
      "  96 XDGUSD_60.csv       : 28425/28440 existing entries\n",
      "  97 XBTUSD_60.csv       : 28425/28440 existing entries\n"
     ]
    }
   ],
   "source": [
    "# Print sorted results\n",
    "for i, (file_name, result) in enumerate(zip(df_USD['file_name'], sorted_results)):\n",
    "    print(f\"{i+1:4d} {file_name:20s}: {result}/{expected_rows} existing entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_USD.iloc[1:].to_csv('USD_60_2022_01_01-2025_03_31.csv', index=False)\n",
    "\n",
    "`USD_60_2022_01_01-2025_03_31.csv`: datasets with 60min, paired with USD, with over 13585/28440 existing timesteps from 2022-01-01 to 2025-03-31 (last date from Kraken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "def csv_to_h5(summary_csv, folder='Kraken_OHLCVT', savepath=None):\n",
    "    \"\"\"\n",
    "    Given a summary CSV path or DataFrame with a 'file_name' column,\n",
    "    saves all the CSVs listed in 'file_name' as datasets in an HDF5 file.\n",
    "\n",
    "    Parameters:\n",
    "        summary_csv: str or pd.DataFrame\n",
    "            Path to the summary CSV or a DataFrame containing 'file_name' column.\n",
    "        folder: str\n",
    "            Parent folder containing the CSV files. Default is 'Kraken_OHLCVT'.\n",
    "        savepath: str or None\n",
    "            Path to the output HDF5 file. If None:\n",
    "                - If summary_csv is a CSV file, use the same path with .h5 extension.\n",
    "                - Otherwise, use 'selected_df.h5'.\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "\n",
    "    # Load summary DataFrame if a path is given\n",
    "    if isinstance(summary_csv, str):\n",
    "        summary_df = pd.read_csv(summary_csv)\n",
    "        if savepath is None:\n",
    "            base, ext = os.path.splitext(summary_csv)\n",
    "            savepath = base + '.h5'\n",
    "    else:\n",
    "        summary_df = summary_csv\n",
    "        if savepath is None:\n",
    "            savepath = 'selected_df.h5'\n",
    "\n",
    "    file_names = summary_df['file_name'].tolist()\n",
    "\n",
    "    with h5py.File(savepath, 'w') as h5f:\n",
    "        for idx, fname in enumerate(file_names):\n",
    "            csv_path = os.path.join(folder, fname) if folder else fname\n",
    "            df = pd.read_csv(csv_path)\n",
    "            dset_name = str(idx)\n",
    "            if all([pd.api.types.is_numeric_dtype(df[col]) for col in df.columns]):\n",
    "                h5f.create_dataset(dset_name, data=df.values)\n",
    "            else:\n",
    "                data_as_str = df.astype(str).values.astype('S')\n",
    "                h5f.create_dataset(dset_name, data=data_as_str)\n",
    "            h5f[dset_name].attrs['file_name'] = fname\n",
    "# Example usage:\n",
    "# csv_to_h5('/Users/xin/Stanford/MSE 349 Financial Statistics/crypto_data/USD_60_2022_01_01-2025_03_31.csv')\n",
    "\n",
    "\n",
    "# Path to your HDF5 file (change as needed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasets in an HDF5 file are not inherently ordered — they are accessed like items in a dictionary, by their key (name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets in this HDF5 file:\n",
      "Dataset: 0, file_name: BNTUSD_60.csv\n",
      "Dataset: 1, file_name: REPUSD_60.csv\n",
      "Dataset: 10, file_name: CTSIUSD_60.csv\n",
      "Dataset: 11, file_name: KARUSD_60.csv\n",
      "Dataset: 12, file_name: BNCUSD_60.csv\n",
      "Dataset: 13, file_name: BANDUSD_60.csv\n",
      "Dataset: 14, file_name: KEEPUSD_60.csv\n",
      "Dataset: 15, file_name: OGNUSD_60.csv\n",
      "Dataset: 16, file_name: LSKUSD_60.csv\n",
      "Dataset: 17, file_name: REPV2USD_60.csv\n",
      "Dataset: 18, file_name: GNOUSD_60.csv\n",
      "Dataset: 19, file_name: KNCUSD_60.csv\n",
      "Dataset: 2, file_name: GHSTUSD_60.csv\n",
      "Dataset: 20, file_name: MLNUSD_60.csv\n",
      "Dataset: 21, file_name: ICXUSD_60.csv\n",
      "Dataset: 22, file_name: RARIUSD_60.csv\n",
      "Dataset: 23, file_name: YFIUSD_60.csv\n",
      "Dataset: 24, file_name: MIRUSD_60.csv\n",
      "Dataset: 25, file_name: LPTUSD_60.csv\n",
      "Dataset: 26, file_name: CQTUSD_60.csv\n",
      "Dataset: 27, file_name: KILTUSD_60.csv\n",
      "Dataset: 28, file_name: RENUSD_60.csv\n",
      "Dataset: 29, file_name: QTUMUSD_60.csv\n",
      "Dataset: 3, file_name: SDNUSD_60.csv\n",
      "Dataset: 30, file_name: OXTUSD_60.csv\n",
      "Dataset: 31, file_name: SRMUSD_60.csv\n",
      "Dataset: 32, file_name: ZRXUSD_60.csv\n",
      "Dataset: 33, file_name: PHAUSD_60.csv\n",
      "Dataset: 34, file_name: STORJUSD_60.csv\n",
      "Dataset: 35, file_name: ANKRUSD_60.csv\n",
      "Dataset: 36, file_name: OMGUSD_60.csv\n",
      "Dataset: 37, file_name: MOVRUSD_60.csv\n",
      "Dataset: 38, file_name: LRCUSD_60.csv\n",
      "Dataset: 39, file_name: SUSHIUSD_60.csv\n",
      "Dataset: 4, file_name: BADGERUSD_60.csv\n",
      "Dataset: 40, file_name: COMPUSD_60.csv\n",
      "Dataset: 41, file_name: BATUSD_60.csv\n",
      "Dataset: 42, file_name: 1INCHUSD_60.csv\n",
      "Dataset: 43, file_name: DYDXUSD_60.csv\n",
      "Dataset: 44, file_name: CHZUSD_60.csv\n",
      "Dataset: 45, file_name: MKRUSD_60.csv\n",
      "Dataset: 46, file_name: DASHUSD_60.csv\n",
      "Dataset: 47, file_name: AXSUSD_60.csv\n",
      "Dataset: 48, file_name: INJUSD_60.csv\n",
      "Dataset: 49, file_name: NANOUSD_60.csv\n",
      "Dataset: 5, file_name: PERPUSD_60.csv\n",
      "Dataset: 50, file_name: PAXGUSD_60.csv\n",
      "Dataset: 51, file_name: ENJUSD_60.csv\n",
      "Dataset: 52, file_name: ETCUSD_60.csv\n",
      "Dataset: 53, file_name: OCEANUSD_60.csv\n",
      "Dataset: 54, file_name: ZECUSD_60.csv\n",
      "Dataset: 55, file_name: SNXUSD_60.csv\n",
      "Dataset: 56, file_name: CRVUSD_60.csv\n",
      "Dataset: 57, file_name: EWTUSD_60.csv\n",
      "Dataset: 58, file_name: SCUSD_60.csv\n",
      "Dataset: 59, file_name: KAVAUSD_60.csv\n",
      "Dataset: 6, file_name: OXYUSD_60.csv\n",
      "Dataset: 60, file_name: DAIUSD_60.csv\n",
      "Dataset: 61, file_name: SANDUSD_60.csv\n",
      "Dataset: 62, file_name: MANAUSD_60.csv\n",
      "Dataset: 63, file_name: MINAUSD_60.csv\n",
      "Dataset: 64, file_name: XTZUSD_60.csv\n",
      "Dataset: 65, file_name: EOSUSD_60.csv\n",
      "Dataset: 66, file_name: FLOWUSD_60.csv\n",
      "Dataset: 67, file_name: AUDUSD_60.csv\n",
      "Dataset: 68, file_name: GRTUSD_60.csv\n",
      "Dataset: 69, file_name: AAVEUSD_60.csv\n",
      "Dataset: 7, file_name: WBTCUSD_60.csv\n",
      "Dataset: 70, file_name: FILUSD_60.csv\n",
      "Dataset: 71, file_name: KSMUSD_60.csv\n",
      "Dataset: 72, file_name: UNIUSD_60.csv\n",
      "Dataset: 73, file_name: BCHUSD_60.csv\n",
      "Dataset: 74, file_name: XRPUSD_60.csv\n",
      "Dataset: 75, file_name: LUNAUSD_60.csv\n",
      "Dataset: 76, file_name: ALGOUSD_60.csv\n",
      "Dataset: 77, file_name: AVAXUSD_60.csv\n",
      "Dataset: 78, file_name: XLMUSD_60.csv\n",
      "Dataset: 79, file_name: SHIBUSD_60.csv\n",
      "Dataset: 8, file_name: BALUSD_60.csv\n",
      "Dataset: 80, file_name: TRXUSD_60.csv\n",
      "Dataset: 81, file_name: ADAUSD_60.csv\n",
      "Dataset: 82, file_name: LINKUSD_60.csv\n",
      "Dataset: 83, file_name: ATOMUSD_60.csv\n",
      "Dataset: 84, file_name: USDCUSD_60.csv\n",
      "Dataset: 85, file_name: LTCUSD_60.csv\n",
      "Dataset: 86, file_name: XMRUSD_60.csv\n",
      "Dataset: 87, file_name: USDTUSD_60.csv\n",
      "Dataset: 88, file_name: MATICUSD_60.csv\n",
      "Dataset: 89, file_name: EURUSD_60.csv\n",
      "Dataset: 9, file_name: RAYUSD_60.csv\n",
      "Dataset: 90, file_name: GBPUSD_60.csv\n",
      "Dataset: 91, file_name: DOTUSD_60.csv\n",
      "Dataset: 92, file_name: SOLUSD_60.csv\n",
      "Dataset: 93, file_name: ETHUSD_60.csv\n",
      "Dataset: 94, file_name: XDGUSD_60.csv\n",
      "Dataset: 95, file_name: XBTUSD_60.csv\n"
     ]
    }
   ],
   "source": [
    "h5path = '/Users/xin/Stanford/MSE 349 Financial Statistics/crypto_data/USD_60_2022_01_01-2025_03_31.h5'\n",
    "with h5py.File(h5path, 'r') as h5f:\n",
    "    print(\"Datasets in this HDF5 file:\")\n",
    "    for dset_name in h5f:\n",
    "        file_name = h5f[dset_name].attrs.get('file_name', None)\n",
    "        print(f\"Dataset: {dset_name}, file_name: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def add_missing_row_info(summary_csv, timestep=3600):\n",
    "    \"\"\"\n",
    "    Adds number_missing and ratio_missing columns to the summary CSV file,\n",
    "    assuming uniform time intervals between first_unix_time and last_unix_time.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(summary_csv)\n",
    "\n",
    "    # Compute expected rows from the time range and timestep\n",
    "    df['expected_rows'] = (df['last_unix_time'] - df['first_unix_time']) // timestep + 1\n",
    "    missing_rows = df['expected_rows'] - df['number_rows']\n",
    "    df['ratio_missing'] = missing_rows / df['expected_rows']\n",
    "\n",
    "    # Drop helper column if not needed\n",
    "    df.drop(columns=['expected_rows'], inplace=True)\n",
    "\n",
    "    # Overwrite the original file\n",
    "    #df.to_csv(summary_csv, index=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = add_missing_row_info('USD_60_2022_01_01-2025_03_31_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>first_unix_time</th>\n",
       "      <th>last_unix_time</th>\n",
       "      <th>first_date</th>\n",
       "      <th>last_date</th>\n",
       "      <th>number_rows</th>\n",
       "      <th>ratio_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BNTUSD_60.csv</td>\n",
       "      <td>1621868400</td>\n",
       "      <td>1743451200</td>\n",
       "      <td>2021-05-24 15:00:00</td>\n",
       "      <td>2025-03-31 20:00:00</td>\n",
       "      <td>16000</td>\n",
       "      <td>0.526263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REPUSD_60.csv</td>\n",
       "      <td>1475611200</td>\n",
       "      <td>1743458400</td>\n",
       "      <td>2016-10-04 20:00:00</td>\n",
       "      <td>2025-03-31 22:00:00</td>\n",
       "      <td>45977</td>\n",
       "      <td>0.382054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GHSTUSD_60.csv</td>\n",
       "      <td>1621263600</td>\n",
       "      <td>1743462000</td>\n",
       "      <td>2021-05-17 15:00:00</td>\n",
       "      <td>2025-03-31 23:00:00</td>\n",
       "      <td>17687</td>\n",
       "      <td>0.478951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SDNUSD_60.csv</td>\n",
       "      <td>1630594800</td>\n",
       "      <td>1743462000</td>\n",
       "      <td>2021-09-02 15:00:00</td>\n",
       "      <td>2025-03-31 23:00:00</td>\n",
       "      <td>17614</td>\n",
       "      <td>0.438204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BADGERUSD_60.csv</td>\n",
       "      <td>1628002800</td>\n",
       "      <td>1743451200</td>\n",
       "      <td>2021-08-03 15:00:00</td>\n",
       "      <td>2025-03-31 20:00:00</td>\n",
       "      <td>18982</td>\n",
       "      <td>0.408107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PERPUSD_60.csv</td>\n",
       "      <td>1626188400</td>\n",
       "      <td>1743462000</td>\n",
       "      <td>2021-07-13 15:00:00</td>\n",
       "      <td>2025-03-31 23:00:00</td>\n",
       "      <td>19777</td>\n",
       "      <td>0.392915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OXYUSD_60.csv</td>\n",
       "      <td>1632841200</td>\n",
       "      <td>1743458400</td>\n",
       "      <td>2021-09-28 15:00:00</td>\n",
       "      <td>2025-03-31 22:00:00</td>\n",
       "      <td>18458</td>\n",
       "      <td>0.399310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>WBTCUSD_60.csv</td>\n",
       "      <td>1628010000</td>\n",
       "      <td>1743462000</td>\n",
       "      <td>2021-08-03 17:00:00</td>\n",
       "      <td>2025-03-31 23:00:00</td>\n",
       "      <td>17874</td>\n",
       "      <td>0.442674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BALUSD_60.csv</td>\n",
       "      <td>1600351200</td>\n",
       "      <td>1743433200</td>\n",
       "      <td>2020-09-17 14:00:00</td>\n",
       "      <td>2025-03-31 15:00:00</td>\n",
       "      <td>27349</td>\n",
       "      <td>0.311906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RAYUSD_60.csv</td>\n",
       "      <td>1632841200</td>\n",
       "      <td>1743462000</td>\n",
       "      <td>2021-09-28 15:00:00</td>\n",
       "      <td>2025-03-31 23:00:00</td>\n",
       "      <td>20742</td>\n",
       "      <td>0.325002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_name  first_unix_time  last_unix_time           first_date  \\\n",
       "0     BNTUSD_60.csv       1621868400      1743451200  2021-05-24 15:00:00   \n",
       "1     REPUSD_60.csv       1475611200      1743458400  2016-10-04 20:00:00   \n",
       "2    GHSTUSD_60.csv       1621263600      1743462000  2021-05-17 15:00:00   \n",
       "3     SDNUSD_60.csv       1630594800      1743462000  2021-09-02 15:00:00   \n",
       "4  BADGERUSD_60.csv       1628002800      1743451200  2021-08-03 15:00:00   \n",
       "5    PERPUSD_60.csv       1626188400      1743462000  2021-07-13 15:00:00   \n",
       "6     OXYUSD_60.csv       1632841200      1743458400  2021-09-28 15:00:00   \n",
       "7    WBTCUSD_60.csv       1628010000      1743462000  2021-08-03 17:00:00   \n",
       "8     BALUSD_60.csv       1600351200      1743433200  2020-09-17 14:00:00   \n",
       "9     RAYUSD_60.csv       1632841200      1743462000  2021-09-28 15:00:00   \n",
       "\n",
       "             last_date  number_rows  ratio_missing  \n",
       "0  2025-03-31 20:00:00        16000       0.526263  \n",
       "1  2025-03-31 22:00:00        45977       0.382054  \n",
       "2  2025-03-31 23:00:00        17687       0.478951  \n",
       "3  2025-03-31 23:00:00        17614       0.438204  \n",
       "4  2025-03-31 20:00:00        18982       0.408107  \n",
       "5  2025-03-31 23:00:00        19777       0.392915  \n",
       "6  2025-03-31 22:00:00        18458       0.399310  \n",
       "7  2025-03-31 23:00:00        17874       0.442674  \n",
       "8  2025-03-31 15:00:00        27349       0.311906  \n",
       "9  2025-03-31 23:00:00        20742       0.325002  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process CSV Files & Compute Log Returns\n",
    "\n",
    "- Add column names\n",
    "- Truncate to desired timeframe\n",
    "- Add index (in 0,1,2,...)\n",
    "- Find missing indices\n",
    "- Add log returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_colnames(input_data, column_names=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'trades']):\n",
    "    \"\"\"\n",
    "    Add column names to a DataFrame without a header, from a CSV file or an existing DataFrame.\n",
    "    input_data: String (CSV file path) or pandas DataFrame.\n",
    "    \"\"\"\n",
    "    # Check if input_data is a string (CSV path)\n",
    "    if isinstance(input_data, str):\n",
    "        df = pd.read_csv(input_data, header=None)\n",
    "    # Check if input_data is a DataFrame\n",
    "    elif isinstance(input_data, pd.DataFrame):\n",
    "        df = input_data.copy()  # Avoid modifying the original\n",
    "    # Validate number of columns\n",
    "    num_cols = df.shape[1]\n",
    "    num_expected = len(column_names)\n",
    "    \n",
    "    if num_cols != num_expected:\n",
    "        raise ValueError(f\"DataFrame has {num_cols} columns, but {num_expected} column names were provided: {column_names}\")\n",
    "    df.columns = column_names\n",
    "    return df\n",
    "\n",
    "def add_index_truncate(df, start_time, end_time, timestep):\n",
    "    \"\"\"\n",
    "    Adds an 'index' column (in range(T)) \n",
    "    Truncates the df to only keep the range [start_time-timestep, end_time] # keep -1 for return computation\n",
    "    Return the df with a list of missing indices\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Truncate to only keep rows within [start_time-timestep, end_time] (need -1 entry to compute return)\n",
    "    df = df[(df['timestamp'] >= start_time-timestep) & (df['timestamp'] <= end_time)].reset_index(drop=True)\n",
    "    df['index'] = ((df['timestamp'] - start_time) // timestep).astype(int)\n",
    "    T = int((end_time - start_time) // timestep) + 1\n",
    "    missing_indices = sorted(list(set(range(T)) - set(df[1:]['index'])))\n",
    "    return df, missing_indices\n",
    "\n",
    "\n",
    "def add_log_return(df):\n",
    "    \"\"\"\n",
    "    Input: df with 'index' and 'close' columns.\n",
    "    Add a 'log_return' column to the df\n",
    "    Delete index -1 to have time within [start_time, end_time]\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['log_return'] = np.nan\n",
    "\n",
    "    if df.loc[0,'index'] == -1:\n",
    "        if not np.isnan(df.loc[0,'close']) and not np.isnan(df.loc[1,'close']):\n",
    "            df.loc[1,'log_return'] = np.log(df.loc[0,'close'] / df.loc[1,'close'])\n",
    "        # Remove the first row and reset index\n",
    "        df = df.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        prev_close = df.loc[i-1, 'close']\n",
    "        curr_close = df.loc[i, 'close']\n",
    "        if df.loc[i,'index'] == df.loc[i-1,'index'] + 1 and not np.isnan(prev_close) and not np.isnan(curr_close):\n",
    "            df.loc[i,'log_return'] = np.log(curr_close / prev_close)\n",
    "        else:\n",
    "            df.loc[i,'log_return'] = np.nan\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1640995200 1743379200\n",
      "2022-01-01 00:00:00 2025-03-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# # Set timeframe\n",
    "start_day=\"2022-01-01\"\n",
    "end_day=\"2025-03-31\"\n",
    "start_unix = int(pd.to_datetime(start_day).timestamp())\n",
    "end_unix = int(pd.to_datetime(end_day).timestamp())\n",
    "print(start_unix, end_unix)\n",
    "print(pd.to_datetime(start_unix, unit='s'), pd.to_datetime(end_unix, unit='s'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncate the datasets, add headers, index column, log returns, and save them to a new h5 file indexed by 0,1...N, save missing indices (list of lists) to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.width', 1000) \n",
    "df = pd.read_csv('USD_60_2022_01_01-2025_03_31.csv')\n",
    "filepaths = [f'Kraken_OHLCVT/{filename}' for filename in df['file_name']]\n",
    "\n",
    "# # Save filepaths to a text file in a readable format\n",
    "# savepath = 'USD_60_2022_01_01-2025_03_31_filenames.txt'\n",
    "# with open(savepath, 'w') as f:\n",
    "#     for path in filepaths:\n",
    "#         f.write(f\"{path}\\n\")\n",
    "# # Read the file and convert each line into a string in a list\n",
    "# with open(\"USD_60_2022_01_01-2025_03_31_filenames.txt\", \"r\") as file:\n",
    "#     filepaths = [line.strip() for line in file if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    timestamp    open    high     low   close       volume  trades  index  log_return\n",
      "0  1640995200  18.183  18.950  18.183  18.768   472.512747      39      0   -0.032657\n",
      "1  1640998800  18.870  18.870  18.870  18.870    51.825791       2      1    0.005420\n",
      "2  1641002400  19.191  19.229  19.047  19.047   424.269850      38      2    0.009336\n",
      "3  1641006000  19.251  19.453  19.229  19.366   224.133773      21      3    0.016609\n",
      "4  1641009600  19.381  19.509  19.014  19.200  1805.804939      37      4   -0.008609\n",
      "5  1641013200  19.200  19.283  18.848  19.000    41.684101       8      5   -0.010471\n",
      "6  1641027600  18.770  18.790  18.705  18.705     4.540137       6      9         NaN\n",
      "7  1641031200  18.778  18.778  18.689  18.689   354.129610      12     10   -0.000856\n",
      "8  1641056400  18.766  18.766  18.766  18.766     2.618940       1     17         NaN\n",
      "9  1641060000  18.698  18.698  18.698  18.698   235.519170       2     18   -0.003630\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv(filepaths[1], header=None)\n",
    "df2 = add_colnames(df2)\n",
    "df22, missing2 = add_index_truncate(df2, start_unix, end_unix, 3600)\n",
    "df22 = add_log_return(df22)\n",
    "print(df22.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyarrow # pyarrow is required for parquet support\n",
    "# !pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate datasets in filepaths to desired timeframe and save them as parquet files to output_dir\n",
    "# parquet is the most efficient way of saving dfs, it's much faster to process than csv or h5 and takes less storage\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "output_dir = \"USD_60_2022-2025\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "list_missing = []\n",
    "for fp in filepaths:\n",
    "    stock_name = os.path.splitext(os.path.basename(fp))[0]  # e.g., \"AAPL\"\n",
    "    df = pd.read_csv(fp)\n",
    "    df = add_colnames(df)\n",
    "    df, missing = add_index_truncate(df, start_unix, end_unix, 3600)\n",
    "    df = add_log_return(df)\n",
    "    list_missing.append(missing)\n",
    "    df.to_parquet(f\"{output_dir}/{stock_name}.parquet\", index=False)\n",
    "\n",
    "# load parquet file\n",
    "# dfp = pd.read_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
